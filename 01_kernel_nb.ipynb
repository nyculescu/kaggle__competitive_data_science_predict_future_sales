{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bit16146ae4c3824408ac799370af011b09",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame the problem and look at the big picture"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The objective: what needs to be done?\n",
    "The task is to forecast the total amount of products sold in every shop for the test set.\n",
    "\n",
    "! Note that the list of shops and products slightly changes every month.\n",
    "\n",
    "## Frame this problem\n",
    "* typical supervised learning task - we have the labeled training examples\n",
    "* typical regression task - we're asked to predict a value\n",
    "    * multiple regression problem (value prediction) - the system will use multiple features to make a prediction\n",
    "    * also univariate regression problem - we're only trying to predict a single value (*total amount of products sold*) in every *shop*\n",
    "* plain batch learning - we don't have a continuous flow of data coming to the system - the data doesn't need to be adjusted rapidly, and the data is small enoug to fit in memory (`is it so?`)\n",
    "\n",
    "## How should performance be measured?\n",
    "`todo`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sales_train = pd.read_csv(os.path.abspath(os.path.join('input', 'sales_train.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train.head()\n",
    "sales_train.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the amount of memory used (in bytes) for each column. \n",
    "# It’s useful when building machine learning models which may require a lot memory in training.\n",
    "sales_train.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can find out what categories exist and how many districts belong to each category by using the value_counts() method, but we know them already (form the description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Var 1\n",
    "# For numeric data, the result’s index will include count, mean, std, min, max as well as lower, 50 and upper percentiles.\n",
    "# For object data (e.g. strings or timestamps), the result’s index will include count, unique, top, and freq. The top is the most common value. The freq is the most common value’s frequency. Timestamps also include the first and last items.\n",
    "\n",
    "sales_train.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"date\" field doesn't look to be relevant, so drop it\n",
    "sales_train.drop(\"date\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Var 2 - print the entire number in Python from describe() function, but only for the numeric data\n",
    "\n",
    "def describe_entire_nrs(datafr):\n",
    "    desc = datafr.describe()\n",
    "    desc.loc['count'] = desc.loc['count'].astype(int).astype(str)\n",
    "    desc.iloc[1:] = desc.iloc[1:].applymap('{:.3f}'.format)\n",
    "    return desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(describe_entire_nrs(sales_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "sales_train.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations on th plotted dataframe:\n",
    "* date_block_num, item_id, shop_id are capped -- Warning from hands-on book: The latter may be a serious problem since it is your target attribute (your labels). Your Machine Learning algorithms may learn that prices never go beyond that limit.\n",
    "    * date_block_num even capped, doesn't look to be a problem\n",
    "* These attributes have very different scales\n",
    "* item_cnt_day and item_price don't express anything\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Compute a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a hash of each instance’s identifier and put that instance in the test set if the hash is lower or equal to 20% of the maximum hash value. This ensures that the test set will remain consistent across multiple runs, even if you refresh the dataset. The new test set will contain 20% of the new instances, but it will not contain any instance that was previously in the training set\n",
    "from zlib import crc32\n",
    "\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train_with_id = sales_train.reset_index()   # adds an `index` column\n",
    "# sales_train_with_id[\"id\"] = sales_train[\"shop_id\"] * 100 + sales_train[\"item_id\"] # (*1) the shop_id doesn't exceed value 99 for this project\n",
    "\n",
    "train_set, test_set = split_train_test_by_id(sales_train_with_id, 0.2, \"index\") # if (*1) is used, replace \"index\" with \"id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(describe_entire_nrs(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "### the other data from the CSVs &darr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_categories = pd.read_csv(os.path.abspath(os.path.join('input', 'item_categories.csv')))\n",
    "items = pd.read_csv(os.path.abspath(os.path.join('input', 'items.csv')))\n",
    "sample_submission = pd.read_csv(os.path.abspath(os.path.join('input', 'sample_submission.csv')))\n",
    "shops = pd.read_csv(os.path.abspath(os.path.join('input', 'shops.csv')))\n",
    "test = pd.read_csv(os.path.abspath(os.path.join('input', 'test.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_categories.info()\n",
    "item_categories.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.info()\n",
    "items.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.info()\n",
    "sample_submission.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shops.info()\n",
    "shops.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()\n",
    "test.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the other data from the CSVs &uarr;\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.axes._subplots.AxesSubplot at 0x2316381e708>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "ERROR:root:Internal Python error in the inspect module.\nBelow is the traceback from this internal error.\n\nError in callback <function flush_figures at 0x000002314BC3E558> (for post_execute):\n\nKeyboardInterrupt\n\n"
    }
   ],
   "source": [
    "train_set.plot(kind=\"scatter\", x=\"date_block_num\", y=\"item_cnt_day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input files\n",
    "\n",
    "* shops.csv- supplemental information about the shops -- `61 entries`\n",
    "    * shop_name (e.g., \"СПб ТК \"\"Сенная\"\"\")\n",
    "    * shop_id (e.g., 43)\n",
    "\n",
    "* item_categories.csv  - supplemental information about the items categories -- `85 entries`\n",
    "    * item_category_name (e.g., Кино - DVD)\n",
    "    * item_category_id (e.g., 40)\n",
    "\n",
    "* items.csv - supplemental information about the items/products -- `22.171 entries`\n",
    "    * item_name (e.g., 1812: 4 СЕРИИ (регион))\n",
    "    * item_id (e.g., 97)\n",
    "    * item_category_id (e.g., 40)\n",
    "\n",
    "* sales_train.csv - the training set. Daily historical data from January 2013 to October 2015 -- `2.935.850 entries | 587.170 entries should be allotted to the training set`\n",
    "    * <strike>date (e.g., 23.02.2013)</strike> *I don't see the reason of using this in ML training because we already have date_block_num as an attribute*\n",
    "    * date_block_num (e.g., 1)\n",
    "    * shop_id (e.g., 43) - `shop_id and item_id shall be concatenated to ID`\n",
    "    * item_id (e.g., 97) - `shop_id and item_id shall be concatenated to ID`\n",
    "    * item_price (e.g., 149.0)\n",
    "    * item_cnt_day (e.g., 1.0)\n",
    "\n",
    "* sample_submission.csv - a sample submission file in the correct format -- `214.201 entries`\n",
    "    * ID (e.g., 0)\n",
    "    * item_cnt_month (e.g., 0.5)\n",
    "\n",
    "* test.csv - the test set. You need to forecast the sales for these shops and products for November 2015 -- `214.201 entries`\n",
    "    * ID (e.g., 0)\n",
    "    * shop_id (e.g., 43)\n",
    "    * item_id (e.g., 97)\n",
    "\n",
    "## Data fields\n",
    "* ID - an Id that represents a (Shop, Item) tuple within the test set\n",
    "* shop_id - unique identifier of a shop\n",
    "* item_id - unique identifier of a product\n",
    "* item_category_id - unique identifier of item category\n",
    "* item_cnt_day - number of products sold. You are predicting a monthly amount of this measure\n",
    "* item_price - current price of an item\n",
    "* date - date in format dd/mm/yyyy\n",
    "* date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n",
    "* item_name - name of item\n",
    "* shop_name - name of shop\n",
    "* item_category_name - name of item category\n",
    "\n",
    "## Data format\n",
    "`todo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a test set, put it aside, and never look at it (no data snooping!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* Study each attribute and its characteristics:\n",
    "    * Name\n",
    "    * Type (categorical, int/float, bounded/unbounded, text, structured, etc.)\n",
    "    * % of missing values\n",
    "    * Noisiness and type of noise (stochastic, outliers, rounding errors, etc.)\n",
    "    * Possibly useful for the task?\n",
    "    * Type of distribution (Gaussian, uniform, logarithmic, etc.)\n",
    "* For supervised learning tasks, identify the target attribute(s).\n",
    "* Visualize the data.\n",
    "* Study the correlations between attributes.\n",
    "* Study how you would solve the problem manually.\n",
    "* Identify the promising transformations you may want to apply.\n",
    "* Identify extra data that would be useful (go back to “Get the Data”).\n",
    "* Document what you have learned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data cleaning\n",
    "* Fix or remove outliers (optional).\n",
    "* Fill in missing values (e.g., with zero, mean, median…) or drop their rows (or columns).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature selection\n",
    "* Drop the attributes that provide no useful information for the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature engineering\n",
    "* Discretize continuous features.\n",
    "* Decompose features (e.g., categorical, date/time, etc.).\n",
    "* Add promising transformations of features (e.g., log(x), sqrt(x), x2, etc.).\n",
    "* Aggregate features into promising new features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature scaling: standardize or normalize features."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short-List Promising Models\n",
    "* Train many quick and dirty models from different categories (e.g., linear, naive Bayes, SVM, Random Forests, neural net, etc.) using standard parameters.\n",
    "* Measure and compare their performance.\n",
    "* For each model, use N-fold cross-validation and compute the mean and standard deviation of the performance measure on the N folds.\n",
    "* Analyze the most significant variables for each algorithm.\n",
    "* Analyze the types of errors the models make.\n",
    "* What data would a human have used to avoid these errors?\n",
    "* Have a quick round of feature selection and engineering.\n",
    "* Have one or two more quick iterations of the five previous steps.\n",
    "* Short-list the top three to five most promising models, preferring models that make different types of errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tune the System"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! Use as much data as possible for this step, especially as you move toward the end of fine-tuning\n",
    "* Fine-tune the hyperparameters using cross-validation.\n",
    "    * Treat your data transformation choices as hyperparameters, especially when you are not sure about them (e.g., should I replace missing values with zero or with the median value? Or just drop the rows?).\n",
    "    * Unless there are very few hyperparameter values to explore, prefer random search over grid search. If training is very long, you may prefer a Bayesian optimization approach (e.g., using Gaussian process priors, as described by Jasper Snoek, Hugo Larochelle, and Ryan Adams).\n",
    "* Try Ensemble methods. Combining your best models will often perform better than running them individually.\n",
    "* Once you are confident about your final model, measure its performance on the test set to estimate the generalization error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present the Solution & Launch!"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}