{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37432bitmlenvvenv7b5ace3298d04bf3950110100be6543b",
   "display_name": "Python 3.7.4 32-bit ('ml_env': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame the problem and look at the big picture"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The objective: what needs to be done?\n",
    "The task is to forecast the total amount of products sold in every shop for the test set.\n",
    "\n",
    "! Note that the list of shops and products slightly changes every month.\n",
    "\n",
    "## Frame this problem\n",
    "* typical supervised learning task - we have the labeled training examples\n",
    "* typical regression task - we're asked to predict a value\n",
    "    * multiple regression problem (value prediction) - the system will use multiple features to make a prediction\n",
    "    * also univariate regression problem - we're only trying to predict a single value (*total amount of products sold*) in every *shop*\n",
    "* plain batch learning - we don't have a continuous flow of data coming to the system - the data doesn't need to be adjusted rapidly, and the data is small enoug to fit in memory (`is it so?`)\n",
    "\n",
    "## How should performance be measured?\n",
    "`todo`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "ifile = os.path.abspath(os.path.join('input', 'sales_train.csv'))\n",
    "rows = []\n",
    "with open(ifile) as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        rows.append(row)\n",
    "\n",
    "# use str() to avoid Exception has occurred: TypeError can only concatenate str (not \"int\") to str\n",
    "print(\"csv row length: \" + str(readCSV.line_num)) \n",
    "print(\"sizeof(row trasnformed into numpy obj): \" + str(np.array(rows).nbytes) + \" in bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "At this run, python takes about 2.4 GB extra RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>date_block_num</th>\n      <th>shop_id</th>\n      <th>item_id</th>\n      <th>item_price</th>\n      <th>item_cnt_day</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>02.01.2013</td>\n      <td>0</td>\n      <td>59</td>\n      <td>22154</td>\n      <td>999.00</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>03.01.2013</td>\n      <td>0</td>\n      <td>25</td>\n      <td>2552</td>\n      <td>899.00</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>05.01.2013</td>\n      <td>0</td>\n      <td>25</td>\n      <td>2552</td>\n      <td>899.00</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>06.01.2013</td>\n      <td>0</td>\n      <td>25</td>\n      <td>2554</td>\n      <td>1709.05</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>15.01.2013</td>\n      <td>0</td>\n      <td>25</td>\n      <td>2555</td>\n      <td>1099.00</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "         date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n0  02.01.2013               0       59    22154      999.00           1.0\n1  03.01.2013               0       25     2552      899.00           1.0\n2  05.01.2013               0       25     2552      899.00          -1.0\n3  06.01.2013               0       25     2554     1709.05           1.0\n4  15.01.2013               0       25     2555     1099.00           1.0"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sales_train = pd.read_csv(os.path.abspath(os.path.join('input', 'sales_train.csv')))\n",
    "sales_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input files\n",
    "\n",
    "* shops.csv- supplemental information about the shops -- `61 entries`\n",
    "    * shop_name (e.g., \"СПб ТК \"\"Сенная\"\"\")\n",
    "    * shop_id (e.g., 43)\n",
    "\n",
    "* item_categories.csv  - supplemental information about the items categories -- `85 entries`\n",
    "    * item_category_name (e.g., Кино - DVD)\n",
    "    * item_category_id (e.g., 40)\n",
    "\n",
    "* items.csv - supplemental information about the items/products -- `22.171 entries`\n",
    "    * item_name (e.g., 1812: 4 СЕРИИ (регион))\n",
    "    * item_id (e.g., 97)\n",
    "    * item_category_id (e.g., 40)\n",
    "\n",
    "* sales_train.csv - the training set. Daily historical data from January 2013 to October 2015 -- `2.935.850 entries | 587.170 entries should be allotted to the training set`\n",
    "    * <strike>date (e.g., 23.02.2013)</strike> *I don't see the reason of using this in ML training because we already have date_block_num as an attribute*\n",
    "    * date_block_num (e.g., 1)\n",
    "    * shop_id (e.g., 43) - `shop_id and item_id shall be concatenated to ID`\n",
    "    * item_id (e.g., 97) - `shop_id and item_id shall be concatenated to ID`\n",
    "    * item_price (e.g., 149.0)\n",
    "    * item_cnt_day (e.g., 1.0)\n",
    "\n",
    "* sample_submission.csv - a sample submission file in the correct format -- `214.201 entries`\n",
    "    * ID (e.g., 0)\n",
    "    * item_cnt_month (e.g., 0.5)\n",
    "\n",
    "* test.csv - the test set. You need to forecast the sales for these shops and products for November 2015 -- `214.201 entries`\n",
    "    * ID (e.g., 0)\n",
    "    * shop_id (e.g., 43)\n",
    "    * item_id (e.g., 97)\n",
    "\n",
    "## Data fields\n",
    "* ID - an Id that represents a (Shop, Item) tuple within the test set\n",
    "* shop_id - unique identifier of a shop\n",
    "* item_id - unique identifier of a product\n",
    "* item_category_id - unique identifier of item category\n",
    "* item_cnt_day - number of products sold. You are predicting a monthly amount of this measure\n",
    "* item_price - current price of an item\n",
    "* date - date in format dd/mm/yyyy\n",
    "* date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n",
    "* item_name - name of item\n",
    "* shop_name - name of shop\n",
    "* item_category_name - name of item category\n",
    "\n",
    "## Data format\n",
    "`todo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a test set, put it aside, and never look at it (no data snooping!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* Study each attribute and its characteristics:\n",
    "    * Name\n",
    "    * Type (categorical, int/float, bounded/unbounded, text, structured, etc.)\n",
    "    * % of missing values\n",
    "    * Noisiness and type of noise (stochastic, outliers, rounding errors, etc.)\n",
    "    * Possibly useful for the task?\n",
    "    * Type of distribution (Gaussian, uniform, logarithmic, etc.)\n",
    "* For supervised learning tasks, identify the target attribute(s).\n",
    "* Visualize the data.\n",
    "* Study the correlations between attributes.\n",
    "* Study how you would solve the problem manually.\n",
    "* Identify the promising transformations you may want to apply.\n",
    "* Identify extra data that would be useful (go back to “Get the Data”).\n",
    "* Document what you have learned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data cleaning\n",
    "* Fix or remove outliers (optional).\n",
    "* Fill in missing values (e.g., with zero, mean, median…) or drop their rows (or columns).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature selection\n",
    "* Drop the attributes that provide no useful information for the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature engineering\n",
    "* Discretize continuous features.\n",
    "* Decompose features (e.g., categorical, date/time, etc.).\n",
    "* Add promising transformations of features (e.g., log(x), sqrt(x), x2, etc.).\n",
    "* Aggregate features into promising new features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature scaling: standardize or normalize features."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short-List Promising Models\n",
    "* Train many quick and dirty models from different categories (e.g., linear, naive Bayes, SVM, Random Forests, neural net, etc.) using standard parameters.\n",
    "* Measure and compare their performance.\n",
    "* For each model, use N-fold cross-validation and compute the mean and standard deviation of the performance measure on the N folds.\n",
    "* Analyze the most significant variables for each algorithm.\n",
    "* Analyze the types of errors the models make.\n",
    "* What data would a human have used to avoid these errors?\n",
    "* Have a quick round of feature selection and engineering.\n",
    "* Have one or two more quick iterations of the five previous steps.\n",
    "* Short-list the top three to five most promising models, preferring models that make different types of errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tune the System"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! Use as much data as possible for this step, especially as you move toward the end of fine-tuning\n",
    "* Fine-tune the hyperparameters using cross-validation.\n",
    "    * Treat your data transformation choices as hyperparameters, especially when you are not sure about them (e.g., should I replace missing values with zero or with the median value? Or just drop the rows?).\n",
    "    * Unless there are very few hyperparameter values to explore, prefer random search over grid search. If training is very long, you may prefer a Bayesian optimization approach (e.g., using Gaussian process priors, as described by Jasper Snoek, Hugo Larochelle, and Ryan Adams).\n",
    "* Try Ensemble methods. Combining your best models will often perform better than running them individually.\n",
    "* Once you are confident about your final model, measure its performance on the test set to estimate the generalization error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present the Solution & Launch!"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}